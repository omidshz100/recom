# Non-negative Matrix Factorization (NMF) Implementation

A comprehensive Python implementation of Non-negative Matrix Factorization with multiple initialization methods and multiplicative update algorithms.

## Overview

Non-negative Matrix Factorization (NMF) is a dimensionality reduction technique that decomposes a non-negative matrix **A** into two non-negative matrices **W** and **H** such that:

```
A â‰ˆ W Ã— H
```

Where:
- **A** is the original matrix (m Ã— n)
- **W** is the basis matrix (m Ã— k) 
- **H** is the coefficient matrix (k Ã— n)
- **k** is the factorization rank (k < min(m,n))

## Key Features

### ðŸš€ Initialization Methods

#### 1. Random Initialization
- Simple uniform random initialization between 1 and 2
- Fast but may lead to slower convergence
- Good baseline for comparison

#### 2. NNDSVD Initialization (Non-negative Double SVD)
- **Advanced mathematical initialization** using Singular Value Decomposition
- Significantly faster convergence than random initialization
- Preserves the mathematical structure of the original matrix
- **How it works:**
  1. Performs SVD on the original matrix: A = U Ã— S Ã— V^T
  2. Uses the largest singular value and vectors for the first component
  3. For remaining components, splits singular vectors into positive/negative parts
  4. Chooses the combination with stronger norm to maintain non-negativity

### ðŸ”„ Multiplicative Update Algorithm

Implements the standard multiplicative update rules:

```python
# Update H matrix
H *= (W.T @ A) / (W.T @ W @ H + Îµ)

# Update W matrix  
W *= (A @ H.T) / (W @ H @ H.T + Îµ)
```

Where `Îµ` (epsilon) prevents division by zero for numerical stability.

## Code Structure

### Core Functions

| Function | Description |
|----------|-------------|
| `random_initialization(A, rank)` | Initialize W and H matrices randomly |
| `nndsvd_initialization(A, rank)` | Initialize using NNDSVD method |
| `multiplicative_update(A, k, max_iter, init_mode)` | Main NMF algorithm with MU updates |
| `print_matrix_info(matrix, name)` | Display matrix statistics and values |
| `plot_convergence(norms)` | Visualize convergence progress |

### Demo Features

- **Sparse Matrix Testing**: Creates test matrices with ~30% zero values
- **Comparative Analysis**: Shows results from both initialization methods
- **Convergence Tracking**: Monitors Frobenius norm reduction over iterations
- **Reconstruction Visualization**: Displays approximation quality

## Usage

### Basic Usage

```python
import numpy as np
from nmf_Implementation import multiplicative_update, print_matrix_info

# Create your matrix
A = np.random.rand(10, 15)
A[A < 0.3] = 0  # Add sparsity

# Perform NMF with NNDSVD initialization
W, H, norms = multiplicative_update(A, rank=5, max_iter=100, init_mode='nndsvd')

# Check results
print_matrix_info(W, "W Matrix")
print_matrix_info(H, "H Matrix")
print(f"Final reconstruction error: {norms[-1]:.6f}")
```

### Running the Demo

```bash
python nmf_Implementation.py
```

The demo will:
1. Create a 6Ã—8 sparse test matrix
2. Compare random vs NNDSVD initialization
3. Run multiplicative updates for 50 iterations
4. Display convergence results and visualizations

## Mathematical Background

### Why NMF?

- **Non-negativity Constraint**: Ensures meaningful, interpretable results
- **Parts-based Representation**: Learns additive components rather than subtractive
- **Applications**: Recommendation systems, image processing, text mining, bioinformatics

### NNDSVD Advantages

1. **Faster Convergence**: Typically 2-5x faster than random initialization
2. **Better Local Minima**: More likely to find good solutions
3. **Deterministic**: Reproducible results (unlike random initialization)
4. **Mathematical Foundation**: Based on the spectral properties of the matrix

### Convergence Monitoring

The algorithm tracks the **Frobenius norm** of the reconstruction error:

```
||A - WÃ—H||_F = âˆš(Î£áµ¢â±¼(Aáµ¢â±¼ - (WÃ—H)áµ¢â±¼)Â²)
```

Lower values indicate better approximation quality.

## Example Output

```
============================================================
NON-NEGATIVE MATRIX FACTORIZATION (NMF) DEMO
============================================================

Original Matrix A:
  Shape: (6, 8)
  Min: 0.000, Max: 0.944
  Mean: 0.267, Std: 0.318
  Sparsity: 29.2%

----------------------------------------
1. RANDOM INITIALIZATION
----------------------------------------
Reconstruction error: 2.156789

----------------------------------------
2. NNDSVD INITIALIZATION  
----------------------------------------
Reconstruction error: 1.234567

----------------------------------------
3. MULTIPLICATIVE UPDATE ALGORITHM
----------------------------------------
Final reconstruction error: 0.123456
Improvement: 2.156789 â†’ 0.123456 (94.3% reduction)
```

## Dependencies

- **NumPy**: Core matrix operations and linear algebra
- **Matplotlib** (optional): For convergence visualization

```bash
pip install numpy matplotlib
```

## Applications

- **Collaborative Filtering**: User-item recommendation matrices
- **Image Processing**: Facial recognition, object detection
- **Text Mining**: Topic modeling, document clustering
- **Bioinformatics**: Gene expression analysis
- **Audio Processing**: Source separation, music analysis

## Performance Tips

1. **Use NNDSVD initialization** for faster convergence
2. **Monitor convergence** - stop early if error plateaus
3. **Choose appropriate rank** - typically 10-50 for most applications
4. **Handle sparsity** - NMF works well with sparse matrices

## License

This implementation is provided for educational and research purposes.